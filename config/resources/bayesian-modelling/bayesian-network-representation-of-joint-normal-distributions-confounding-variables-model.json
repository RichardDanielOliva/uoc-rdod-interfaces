{
    "title": "Bayesian Network Representation of Joint Normal Distributions - Confounding Variables Model",
    "author": [
        "Denis Papaioannou"
    ],
    "date": "March 21, 2022",
    "resume": "This post is the second of a series discussing the Bayesian Network representation of multivariate normal distributions. In the first post we introduced a cascading regressions model leading to a Bayesian network representation of any joint normal distribution <a class=\"cite-link\" href=\"/resources/bayesian-network-representation-of-joint-normal-distributions-confounding-variables-model#DP22\">[DP22]</a>. A joint normal distribution being fully specified by its mean vector and its covariance matrix is not simple to interact with as its Bayesian network equivalent. Representing a joint normal distribution as a Bayesian network enables visualizing and interact the distribution through the lens of probabilistic graphical models with TKRISK®.We demonstrate in this post a simple yet powerful approach using a confounding variables model.",
    "description": [
        {
            "type": "latex",
            "content": "<div class=\"abstract\"><p>This post is the second of a series discussing the Bayesian Network representation of multivariate normal distributions. In the first post we introduced a cascading regressions model leading to a Bayesian network representation of any joint normal distribution <a class=\"cite-link\" href=\"/resources/bayesian-network-representation-of-joint-normal-distributions-confounding-variables-model#DP22\">[DP22]</a>. A joint normal distribution being fully specified by its mean vector and its covariance matrix is not simple to interact with as its Bayesian network equivalent. Representing a joint normal distribution as a Bayesian network enables visualizing and interact the distribution through the lens of probabilistic graphical models with TKRISK®.</p><p>We demonstrate in this post a simple yet powerful approach using a confounding variables model.</div></p><div class=\"title\">Introduction</div><p>A normal distribution $\\mathcal{N}(m, v)$ is fully specified using two parameters: its mean $m\\in\\mathbb R$ and its variance $v\\in\\mathbb R_+$. Likewise, a $n$-dimensional multivariate normal distribution $\\mathcal{N}(M, V)$ is fully specified by its mean vector $M\\in\\mathbb R^n$ and covariance matrix $V\\in\\mathbb R^n\\times\\mathbb R^n$.</p><p>The latter can also be represented as a Bayesian Network, more specifically as a Gaussian Bayesian network <a class=\"cite-link\" href=\"/resources/bayesian-network-representation-of-joint-normal-distributions-confounding-variables-model#KF09\">[KF09]</a> <a class=\"cite-link\" href=\"/resources/bayesian-network-representation-of-joint-normal-distributions-confounding-variables-model#IC20\">[IC20]</a>.</p><div class=\"title\">Theoretical Results</div><p>In what follows we will be working with vector of random variables $X=(X_1,..., X_n)$ following a multivariate normal distribution $\\mathcal{N}(M, V)$.</p><p>By definition $M = (\\mathbb E[X_i])_{i \\in \\llbracket 1, n\\rrbracket}$ and $V = (cov(X_i, X_j))_{(i,j) \\in \\llbracket 1, n\\rrbracket^2}$.</p><div class=\"subtitle\">Confounding Variables Model</div><div class=\"subsubtitle\">Mathematical Derivation</div><p>Through basic matrix operations we will show how $\\mathcal{N}(M, V)$ can be represented through a confounding variables model which is a particular case of a linear Gaussian Structural Equation Model (SEM) <a class=\"cite-link\" href=\"/resources/bayesian-network-representation-of-joint-normal-distributions-confounding-variables-model#MD11\">[MD11]</a> <a class=\"cite-link\" href=\"/resources/bayesian-network-representation-of-joint-normal-distributions-confounding-variables-model#GP19\">[GP19]</a>, the latter having a direct Gaussian Bayesian Network equivalent.</p><p>Applying LDL decomposition on $V$ yields $V = L D L^T$ with $L$ being unit lower triangular and $D$ diagonal.</p><p>Therefore:$$X - M\\sim L E$$ with $E\\sim \\mathcal{N}(0, D)$.</p><p>Instead of rewriting equation  as in <a class=\"cite-link\" href=\"/resources/bayesian-network-representation-of-joint-normal-distributions-confounding-variables-model#DP22\">[DP22]</a> leading to the cascading regressions model, we can directly interpret $E$ as a set of confounding variables. In writing the regression model as:$$X_i = m_i + \\sum_{j=1}^{i-1}l_{ij}e_j + e_i$$with $L=(l_{ij})_{1\\leq i,j \\leq n}$, we obtain a direct Gaussian Bayesian Network representation where the variables $(e_i)_{1\\leq i \\leq n}$ are confounding, meaning these are not observed, however allow introducing a correlation structure among observed ones ($X_1, ..., X_n$ in our case).</p><p>In this case $Pa(X_i) = \\{e_1, ..., e_{i-1}\\}$.\\\\</p><div class=\"subsubtitle\">Algorithm</div><p><div class=\"procedure\"><div><b>procedure</b>  BUILD-GAUSSIAN-BAYESIAN-NETWORK-FROM-COVARIANCE($M$, $V$)</div><div class=\"tab1\">     Set $\\mathcal{G}$ to an empty graph</div><div class=\"commented-line tab1\"><div class=\"line\"> $L, D \\gets \\textit{LDL}(M, V)$ </div><div class=\"comment\">apply LDL decomposition</div></div><div class=\"tab1\"><b>for </b>$i=1$ to $n$<b> do</b></div><div class=\"commented-line tab2\"><div class=\"line\"> Add $e_i\\sim \\mathcal{N}(0, d_{i,i})$ to $\\mathcal{G}$</div><div class=\"comment\">initialize variables with the right marginal distribution</div></div><div class=\"tab2\">  Add $X_i\\sim \\mathcal{N}(m_i, 0)$ to $\\mathcal{G}$</div><div class=\"commented-line tab2\"><div class=\"line\"> Add $e_i \\xrightarrow{1} X_i$ to $\\mathcal{G}$ </div><div class=\"comment\">add $X_i$''s parents with the right coefficients</div></div><div class=\"tab2\">         $X_i$ \\gets $m_i$</div><div class=\"tab2\"><b>for </b>$j=1$ to $i-1$<b> do</b></div><div class=\"tab3\"><b>if </b>$l_{i,j} \\neq 0$<b> then</b></div><div class=\"tab4\">                 Add $e_j \\xrightarrow{l_{i,j}} X_i$ to $\\mathcal{G}$</div><div class=\"tab3\"><b>end if </b></div><div class=\"tab2\"><b>end for </b></div><div class=\"tab1\"><b>end for </b></div><div class=\"tab1\">     <div class=\"tab0\"><b>return </b>$\\mathcal{G}$</div></div><div class=\"tab-1\"><b>end procedure </b></div></div></p><p>It is important to note that a different order in the variables will lead to a different Bayesian Network structure, representing however the exact same joint distribution.</p><p>Like for the cascading regressions model <a class=\"cite-link\" href=\"/resources/bayesian-network-representation-of-joint-normal-distributions-confounding-variables-model#DP22\">[DP22]</a>, the above approach is simple, computationally efficient and allows Bayesian Networks generation straight from any dataset for which variables are assumed normally distributed.</p><p>The LDL decomposition is here sufficient to obtain all Bayesian network parameters and allows handling any case of covariance matrix (even non definite) and thus any multivariate normal distribution. The simplicity of the algorithm comes at the cost of additional variables - the confounding variables.</p><div class=\"subsubtitle\">Normalization</div><p>It is common to work on standardized normal distributions, i.e. with zero mean and unit variance:<ul>    </li><li> $M=0$    </li><li> $\\textit{Diag}(V) = [1, \\cdots, 1]$</li></ul>In this case covariance and correlation matrices are one and the same. This is a convenient setup which does not imply loss of generality as non-normalized marginals can be recovered by shifting and scaling the normalized ones. Working on normalized space allows for direct comparison of dependencies between variables as these are independent of their actual magnitude.</p><p>For a self-contained graph representation, contrary to the above generic approach, the Gaussian Bayesian Network is insufficient and would require additional deterministic nodes to apply shifting and scaling (shifting and scaling directly a node on the Bayesian Network would propagate on its children and thus alter the joint distribution).</p><p>For sake of simplicity we will present examples on standardized normal distributions only.</p><div class=\"subsubtitle\">Generic Examples</div><p>To visualize and interpret the above approach, we will take basic examples of 4 dimensional multivariate normal distributions.<div class=\"paragraph\"><b>Independent&nbsp&nbsp&nbsp&nbsp</b>Independent variables naturally translate to no connections between nodes, with the correlation matrix being the identity matrix.</div><div class=\"figure-tabular\">$V = \\begin{bmatrix}1 & 0 & 0 & 0 \\\\0 & 1 & 0 & 0 \\\\0 & 0 & 1 & 0\\\\0 & 0 & 0 & 1\\\\\\end{bmatrix}$  $\\implies$  <img src=\"https://websiteprodstoragene.blob.core.windows.net/images/vcv2bn_hv_theoretical_independent.webp\" alt=vcv2bn_hv_theoretical_independent/> </p><p></div><div class=\"image-title\">Bayesian Network resulting from a confounding variables model.</div><div class=\"paragraph\"><b>Sparse&nbsp&nbsp&nbsp&nbsp</b>In this case $X_3$ is independent from all other variables, hence the lack of incoming and outgoing edges.</div><div class=\"figure-tabular\">$V = \\begin{bmatrix}1 & 0.1 & 0 & 0 \\\\0.1 & 1 & 0 & 0.5 \\\\0 & 0 & 1 & 0\\\\0 & 0.5 & 0 & 1\\\\\\end{bmatrix}$  $\\implies$  <img src=\"https://websiteprodstoragene.blob.core.windows.net/images/vcv2bn_hv_theoretical_sparse.webp\" alt=vcv2bn_hv_theoretical_sparse/> </p><p></div><div class=\"image-title\">Bayesian Network resulting from a confounding variables model.</div><div class=\"paragraph\"><b>Dense&nbsp&nbsp&nbsp&nbsp</b>In the case where all variables are correlated to each other, the corresponding network is fully connected.</div><div class=\"figure-tabular\">$V = \\begin{bmatrix}1 & 0.1 & 0.2 & 0.3 \\\\0.1 & 1 & 0.4 & 0.5 \\\\0.2 & 0.4 & 1 & 0.6\\\\0.3 & 0.5 & 0.6 & 1\\\\\\end{bmatrix}$  $\\implies$  <img src=\"https://websiteprodstoragene.blob.core.windows.net/images/vcv2bn_hv_theoretical_dense.webp\" alt=vcv2bn_hv_theoretical_dense/> </p><p></div><div class=\"image-title\">Bayesian Network resulting from a confounding variables model.</div><div class=\"references-title\">References</div><div class=\"ref-content\"><div class=\"ref-reference\">[KF09]</div><div id=\"KF09\">Daphne Koller and Nir Friedman. Probabilistic Graphical Models: Principles and Techniques, pages 251-254. The MIT Press, second edition, 2009.</div></div><div class=\"ref-content\"><div class=\"ref-reference\">[IC20]</div><div id=\"IC20\">Irene Córdoba, Concha Bielza, Pedro Larragaña, Gherardo Varando. Sparse Cholesky Covariance Parametrization for Recovering Latent Structure in Ordered Data. IEEE, 2020.</div></div><div class=\"ref-content\"><div class=\"ref-reference\">[GP19]</div><div id=\"GP19\">Gunwoong Park, Youngwhan Kim. Identifiability of Gaussian Linear Structural Equation Models with Homogeneous and Heterogeneous Error Variances. Journal of the Korean Statistical Society, 2019.</div></div><div class=\"ref-content\"><div class=\"ref-reference\">[MD11]</div><div id=\"MD11\">Mathias Drton, Rina Foygel, Seth Sullivant. Global Identifiability of Linear Structural Equation Models. The Annals of Statistics, 2011.</div></div><div class=\"ref-content\"><div class=\"ref-reference\">[DP22]</div><div id=\"DP22\"><a class=\"cite-link\" href=\"https://www.company.com/resources/bayesian-network-representation-of-joint-normal-distributions-cascading-regressions-model\">Denis Papaioannou. Bayesian Network Representation of Joint Normal Distributions - Cascading Regressions Model. company, 2022</a>.</div></div>"
        }
    ],
    "section": "bayesian-modelling",
    "product": "tkrisk",
    "image": "bayesian-network-representation-of-joint-normal-distributions-confounding-variables-model",
    "icon": "climate",
    "images": [],
    "nextVideo": "bayesian-network-representation-of-joint-normal-distributions-cascading-regressions-model",
    "relatedVideos": ["bayesian-network-representation-of-joint-normal-distributions-cascading-regressions-model"],
    "relatedContent": [
        {
            "title": "Related Products",
            "items": [
                {
                    "title": "TKRISK",
                    "url": "/products/tkrisk"
                }
            ]
        },
        {
            "title": "References",
            "items": [
                {
                    "title": "",
                    "url": ""
                }
            ]
        }
    ]
}